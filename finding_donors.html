<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>3a8d1645b496450b8f87de74a94d86c7</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<section id="introduction-to-machine-learning-with-tensorflow"
class="cell markdown">
<h1>Introduction to Machine Learning with TensorFlow</h1>
<h2 id="project-finding-donors-for-charityml">Project: Finding Donors
for <em>CharityML</em></h2>
</section>
<section id="introduction" class="cell markdown">
<h2>Introduction</h2>
<p>This is my first project of the Inro to Machine Learning with
Tensorflow Nanodegree!</p>
<p>In this project, I will employ several supervised algorithms of my
choice to accurately model individuals' income using data collected from
the 1994 U.S. Census. I will then choose the best candidate algorithm
from preliminary results and further optimize this algorithm to best
model the data. My goal with this implementation is to construct a model
that accurately predicts whether an individual makes more than
$50,000.</p>
<h3 id="context">Context</h3>
<p>This sort of task can arise in a non-profit setting, where
organizations survive on donations. Understanding an individual's income
can help a non-profit better understand how large of a donation to
request, or whether or not they should reach out to begin with. While it
can be difficult to determine an individual's general income bracket
directly from public sources, we can (as we will see) infer this value
from other publically available features.</p>
</section>
<section id="python-version" class="cell markdown">
<h3>Python Version</h3>
<p>Pyhon Version used in this notebook is:
<code>Python 3.12.6</code></p>
</section>
<section id="dataset" class="cell markdown">
<h3>Dataset</h3>
<p>The dataset for this project originates from the <a
href="https://archive.ics.uci.edu/ml/datasets/Census+Income">UCI Machine
Learning Repository</a>. The datset was donated by Ron Kohavi and Barry
Becker, after being published in the article <em>"Scaling Up the
Accuracy of Naive-Bayes Classifiers: A Decision-Tree Hybrid"</em>. The
article by Ron Kohavi <a
href="https://www.aaai.org/Papers/KDD/1996/KDD96-033.pdf">online</a>.
The data we investigate here consists of small changes to the original
dataset, such as removing the <code>fnlwgt</code> feature and records
with missing or ill-formatted entries.</p>
<p><strong>Featureset Exploration</strong></p>
<ul>
<li><strong>age</strong>: continuous.</li>
<li><strong>workclass</strong>: Private, Self-emp-not-inc, Self-emp-inc,
Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.</li>
<li><strong>education</strong>: Bachelors, Some-college, 11th, HS-grad,
Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters,
1st-4th, 10th, Doctorate, 5th-6th, Preschool.</li>
<li><strong>education-num</strong>: continuous.</li>
<li><strong>marital-status</strong>: Married-civ-spouse, Divorced,
Never-married, Separated, Widowed, Married-spouse-absent,
Married-AF-spouse.</li>
<li><strong>occupation</strong>: Tech-support, Craft-repair,
Other-service, Sales, Exec-managerial, Prof-specialty,
Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing,
Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.</li>
<li><strong>relationship</strong>: Wife, Own-child, Husband,
Not-in-family, Other-relative, Unmarried.</li>
<li><strong>race</strong>: Black, White, Asian-Pac-Islander,
Amer-Indian-Eskimo, Other.</li>
<li><strong>sex</strong>: Female, Male.</li>
<li><strong>capital-gain</strong>: continuous.</li>
<li><strong>capital-loss</strong>: continuous.</li>
<li><strong>hours-per-week</strong>: continuous.</li>
<li><strong>native-country</strong>: United-States, Cambodia, England,
Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan,
Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland,
Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic,
Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua,
Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&amp;Tobago, Peru,
Hong, Holand-Netherlands.</li>
</ul>
</section>
<section id="theory" class="cell markdown">
<h2>Theory</h2>
<h3 id="one-hot-encoding">One-hot encoding</h3>
<p>It creates a <em>"dummy"</em> variable for each possible category of
each non-numeric feature. For example, assume <code>someFeature</code>
has three possible entries: <code>A</code>, <code>B</code>, or
<code>C</code>:</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;"></th>
<th style="text-align: center;">someFeature</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">0</td>
<td style="text-align: center;">B</td>
</tr>
<tr class="even">
<td style="text-align: center;">1</td>
<td style="text-align: center;">C</td>
</tr>
<tr class="odd">
<td style="text-align: center;">2</td>
<td style="text-align: center;">A</td>
</tr>
</tbody>
</table>
<p>We then encode this feature into <code>someFeature_A</code>,
<code>someFeature_B</code> and <code>someFeature_C</code>:</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;"></th>
<th style="text-align: center;">someFeature_A</th>
<th style="text-align: center;">someFeature_B</th>
<th style="text-align: center;">someFeature_C</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="even">
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="odd">
<td style="text-align: center;">2</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
<h3 id="metrics-and-the-naive-predictor">Metrics and the Naive
Predictor</h3>
<p><em>CharityML</em>, equipped with their research, knows individuals
that make more than $50,000 are most likely to donate to their charity.
Because of this, <em>CharityML</em> is particularly interested in
predicting who makes more than $50,000 accurately. It would seem that
using <strong>accuracy</strong> as a metric for evaluating a particular
model's performace would be appropriate. Additionally, identifying
someone that <em>does not</em> make more than $50,000 as someone who
does would be detrimental to <em>CharityML</em>, since they are looking
to find individuals willing to donate. Therefore, a model's ability to
precisely predict those that make more than $50,000 is <em>more
important</em> than the model's ability to <strong>recall</strong> those
individuals. We can use <strong>F-beta score</strong> as a metric that
considers both precision and recall:</p>
<p><span class="math display">$$ F_{\beta} = (1 + \beta^2) \cdot
\frac{\text{precision} \cdot \text{recall}}{\left( \beta^2 \cdot
\text{precision} \right) + \text{recall}} $$</span></p>
<p>In particular, when <span
class="math inline"><em>β</em> = 0.5</span>, more emphasis is placed on
precision. This is called the <span
class="math inline"><em>F</em><sub>0.5</sub></span> score (or F-score
for simplicity).</p>
<p>Looking at the distribution of classes (those who make at most
$50,000, and those who make more), it's clear most individuals do not
make more than $50,000. This can greatly affect
<strong>accuracy</strong>, since we could simply say <em>"this person
does not make more than $50,000"</em> and generally be right, without
ever looking at the data! Making such a statement would be called
<strong>naive</strong>, since we have not considered any information to
substantiate the claim. It is always important to consider the <em>naive
prediction</em> for your data, to help establish a benchmark for whether
a model is performing well. That been said, using that prediction would
be pointless: If we predicted all people made less than $50,000,
<em>CharityML</em> would identify no one as donors.</p>
<h4 id="note-recap-of-accuracy-precision-recall">Note: Recap of
accuracy, precision, recall</h4>
<p><strong>Accuracy</strong> measures how often the classifier makes the
correct prediction. It’s the ratio of the number of correct predictions
to the total number of predictions (the number of test data points).</p>
<p><strong>Precision</strong> tells us what proportion of messages we
classified as spam, actually were spam. It is a ratio of true positives
(words classified as spam, and which are actually spam) to all positives
(all words classified as spam, irrespective of whether that was the
correct classificatio), in other words it is the ratio of</p>
<p><span class="math display">$$\text{Precision} = \frac{\text{True
Positives}} {\text{True Positives} + \text{False
Positives}}$$</span></p>
<p><strong>Recall (sensitivity)</strong> tells us what proportion of
messages that actually were spam were classified by us as spam. It is a
ratio of true positives (words classified as spam, and which are
actually spam) to all the words that were actually spam, in other words
it is the ratio of</p>
<p><span class="math display">$$\text{Recall} = \frac{\text{True
Positives}} {\text{True Positives} + \text{False
Negatives}}$$</span></p>
<p>For classification problems that are skewed in their classification
distributions, like in our case, for example, if we had 100 text
messages and only 2 were spam and the remaining 98 weren't, accuracy is
not a very good metric. We could classify 90 messages as not spam
(including the 2 that were spam, but we classify them as not spam, hence
they would be false negatives) and 10 as spam (all 10 false positives)
and still get a reasonably good accuracy score. For such cases,
precision and recall come in very handy. These two metrics can be
combined to get the F1 score and the weighted average (harmonic mean) of
the precision and recall scores. This score can range from 0 to 1, with
1 being the best possible F1 score(we take the harmonic mean when
dealing with ratios).</p>
</section>
<section id="supervised-learning-models" class="cell markdown">
<h3>Supervised Learning Models</h3>
<p><strong>The following are some of the supervised learning models that
are currently available in</strong> <a
href="http://scikit-learn.org/stable/supervised_learning.html">scikit-learn</a>
<strong>that you may choose from:</strong></p>
<ul>
<li>Gaussian Naive Bayes (GaussianNB)</li>
<li>Decision Trees</li>
<li>Ensemble Methods (Bagging, AdaBoost, Random Forest, Gradient
Boosting)</li>
<li>Stochastic Gradient Descent Classifier (SGDC)</li>
<li>K-Nearest Neighbors (KNeighbors)</li>
<li>Support Vector Machines (SVM)</li>
<li>Logistic Regression</li>
</ul>
</section>
<div class="cell markdown">
<hr />
<h2 id="exploring-the-data">Exploring the Data</h2>
<p>The following code cell below load necessary Python libraries and
load the census data.</p>
<p>Note the last column from this dataset, <code>income</code>, will be
the target label (whether an individual makes more than, or at most,
$50,000 annually). All other columns are features about each individual
in the census database.</p>
</div>
<div class="cell code" data-execution_count="3">
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import libraries necessary for this project</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> time <span class="im">import</span> time</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> display <span class="co"># Allows the use of display() for DataFrames</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Import sklearn.preprocessing.StandardScaler</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> MinMaxScaler</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Import train_test_split</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Import two metrics from sklearn - fbeta_score and accuracy_score, &#39;make_scorer&#39;</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> fbeta_score, accuracy_score, make_scorer</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Import &#39;GridSearchCV&#39;</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> GridSearchCV</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Import a supervised learning model that has &#39;feature_importances_&#39;</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestClassifier</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the three supervised learning models from sklearn</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.neighbors <span class="im">import</span> KNeighborsClassifier</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> SVC</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Import functionality for cloning a model</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.base <span class="im">import</span> clone</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Import supplementary visualization code visuals.py</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> visuals <span class="im">as</span> vs</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Pretty display for notebooks</span></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="4">
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the Census dataset</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">&quot;data/census.csv&quot;</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Success - Display the first record</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>display(df.head())</span></code></pre></div>
<div class="output display_data">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>workclass</th>
      <th>education_level</th>
      <th>education-num</th>
      <th>marital-status</th>
      <th>occupation</th>
      <th>relationship</th>
      <th>race</th>
      <th>sex</th>
      <th>capital-gain</th>
      <th>capital-loss</th>
      <th>hours-per-week</th>
      <th>native-country</th>
      <th>income</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>39</td>
      <td>State-gov</td>
      <td>Bachelors</td>
      <td>13.0</td>
      <td>Never-married</td>
      <td>Adm-clerical</td>
      <td>Not-in-family</td>
      <td>White</td>
      <td>Male</td>
      <td>2174.0</td>
      <td>0.0</td>
      <td>40.0</td>
      <td>United-States</td>
      <td>&lt;=50K</td>
    </tr>
    <tr>
      <th>1</th>
      <td>50</td>
      <td>Self-emp-not-inc</td>
      <td>Bachelors</td>
      <td>13.0</td>
      <td>Married-civ-spouse</td>
      <td>Exec-managerial</td>
      <td>Husband</td>
      <td>White</td>
      <td>Male</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>13.0</td>
      <td>United-States</td>
      <td>&lt;=50K</td>
    </tr>
    <tr>
      <th>2</th>
      <td>38</td>
      <td>Private</td>
      <td>HS-grad</td>
      <td>9.0</td>
      <td>Divorced</td>
      <td>Handlers-cleaners</td>
      <td>Not-in-family</td>
      <td>White</td>
      <td>Male</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>40.0</td>
      <td>United-States</td>
      <td>&lt;=50K</td>
    </tr>
    <tr>
      <th>3</th>
      <td>53</td>
      <td>Private</td>
      <td>11th</td>
      <td>7.0</td>
      <td>Married-civ-spouse</td>
      <td>Handlers-cleaners</td>
      <td>Husband</td>
      <td>Black</td>
      <td>Male</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>40.0</td>
      <td>United-States</td>
      <td>&lt;=50K</td>
    </tr>
    <tr>
      <th>4</th>
      <td>28</td>
      <td>Private</td>
      <td>Bachelors</td>
      <td>13.0</td>
      <td>Married-civ-spouse</td>
      <td>Prof-specialty</td>
      <td>Wife</td>
      <td>Black</td>
      <td>Female</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>40.0</td>
      <td>Cuba</td>
      <td>&lt;=50K</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<div class="cell code" data-execution_count="3">
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>df.info()</span></code></pre></div>
<div class="output stream stdout">
<pre><code>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 45222 entries, 0 to 45221
Data columns (total 14 columns):
 #   Column           Non-Null Count  Dtype  
---  ------           --------------  -----  
 0   age              45222 non-null  int64  
 1   workclass        45222 non-null  object 
 2   education_level  45222 non-null  object 
 3   education-num    45222 non-null  float64
 4   marital-status   45222 non-null  object 
 5   occupation       45222 non-null  object 
 6   relationship     45222 non-null  object 
 7   race             45222 non-null  object 
 8   sex              45222 non-null  object 
 9   capital-gain     45222 non-null  float64
 10  capital-loss     45222 non-null  float64
 11  hours-per-week   45222 non-null  float64
 12  native-country   45222 non-null  object 
 13  income           45222 non-null  object 
dtypes: float64(4), int64(1), object(9)
memory usage: 4.8+ MB
</code></pre>
</div>
</div>
<div class="cell markdown">
<blockquote>
<p>In this dataset we have 14 features with diffrent datatypes. They are
all complete regarding the non-null counts.</p>
</blockquote>
</div>
<section id="implementation-data-exploration" class="cell markdown">
<h3>Implementation: Data Exploration</h3>
<p>A cursory investigation of the dataset will determine how many
individuals fit into either group, and will tell us about the percentage
of these individuals making more than $50,000.</p>
</section>
<div class="cell markdown">
<ul>
<li>The total number of records, <code>n_records</code></li>
</ul>
</div>
<div class="cell code" data-execution_count="5">
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Total number of records</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>n_records <span class="op">=</span> df.shape[<span class="dv">0</span>]</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Total number of records: </span><span class="sc">{}</span><span class="st">&quot;</span>.<span class="bu">format</span>(n_records))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Total number of records: 45222
</code></pre>
</div>
</div>
<div class="cell markdown">
<ul>
<li>The number of individuals making more than $50,000 annually,
<code>n_greater_50k</code>.</li>
</ul>
</div>
<div class="cell code" data-execution_count="6">
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>df[<span class="st">&#39;income&#39;</span>].unique() <span class="co">## looking into the values on income</span></span></code></pre></div>
<div class="output execute_result" data-execution_count="6">
<pre><code>array([&#39;&lt;=50K&#39;, &#39;&gt;50K&#39;], dtype=object)</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="6">
<div class="sourceCode" id="cb9"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Number of records where individual&#39;s income is more than $50,000</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>n_greater_50k <span class="op">=</span> df[df[<span class="st">&#39;income&#39;</span>] <span class="op">==</span> <span class="st">&quot;&gt;50K&quot;</span>][<span class="st">&#39;age&#39;</span>].count() <span class="co">## the count of any column in this filtering is the same so I chose the age to count</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Individuals making more than $50,000: </span><span class="sc">{}</span><span class="st">&quot;</span>.<span class="bu">format</span>(n_greater_50k))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Individuals making more than $50,000: 11208
</code></pre>
</div>
</div>
<div class="cell markdown">
<ul>
<li>The number of individuals making at most $50,000 annually,
<code>n_at_most_50k</code>.</li>
</ul>
</div>
<div class="cell code" data-execution_count="7">
<div class="sourceCode" id="cb11"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Number of records where individual&#39;s income is at most $50,000</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>n_at_most_50k <span class="op">=</span> df[df[<span class="st">&#39;income&#39;</span>] <span class="op">==</span> <span class="st">&quot;&lt;=50K&quot;</span>][<span class="st">&#39;age&#39;</span>].count() <span class="co">## the count of any column in this filtering is the same so I chose the age to count</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Individuals making at most $50,000: </span><span class="sc">{}</span><span class="st">&quot;</span>.<span class="bu">format</span>(n_at_most_50k))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Individuals making at most $50,000: 34014
</code></pre>
</div>
</div>
<div class="cell markdown">
<ul>
<li>The percentage of individuals making more than $50,000 annually,
<code>greater_percent</code>.</li>
</ul>
</div>
<div class="cell code" data-execution_count="9">
<div class="sourceCode" id="cb13"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Percentage of individuals whose income is more than $50,000</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>greater_percent <span class="op">=</span> (n_greater_50k <span class="op">/</span> n_records) <span class="op">*</span> <span class="dv">100</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Percentage of individuals making more than $50,000: </span><span class="sc">{}</span><span class="st">%&quot;</span>.<span class="bu">format</span>(greater_percent))</span></code></pre></div>
<div class="output error" data-ename="NameError"
data-evalue="name &#39;n_greater_50k&#39; is not defined">
<pre><code>---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
Cell In[9], line 2
      1 # Percentage of individuals whose income is more than $50,000
----&gt; 2 greater_percent = (n_greater_50k / n_records) * 100
      3 print(&quot;Percentage of individuals making more than $50,000: {}%&quot;.format(greater_percent))

NameError: name &#39;n_greater_50k&#39; is not defined
</code></pre>
</div>
</div>
<div class="cell markdown">
<hr />
<h2 id="preparing-the-data">Preparing the Data</h2>
<p>Before data can be used as input for machine learning algorithms, it
must be cleaned, formatted, and restructured — this is typically known
as <strong>preprocessing</strong>. Fortunately, for this dataset, there
are no invalid or missing entries we must deal with, however, there are
some qualities about certain features that must be adjusted. This
preprocessing can help tremendously with the outcome and predictive
power of nearly all learning algorithms.</p>
</div>
<section id="transforming-skewed-continuous-features"
class="cell markdown">
<h3>Transforming Skewed Continuous Features</h3>
<p>A dataset may sometimes contain at least one feature whose values
tend to lie near a single number, but will also have a non-trivial
number of vastly larger or smaller values than that single number.
Algorithms can be sensitive to such distributions of values and can
underperform if the range is not properly normalized. With the census
dataset two features fit this description: <code>capital-gain</code> and
<code>capital-loss</code>.</p>
<p>Run the code cell below to plot a histogram of these two features.
Note the range of the values present and how they are distributed.</p>
</section>
<div class="cell code" data-execution_count="10">
<div class="sourceCode" id="cb15"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data into features and target label</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> df[<span class="st">&#39;income&#39;</span>] <span class="co">## I renamed the target feature as convention that is Y</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df.drop(<span class="st">&#39;income&#39;</span>, axis <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize skewed continuous features of original data</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>vs.distribution(df)</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>plt.show()<span class="op">;</span></span></code></pre></div>
<div class="output stream stderr">
<pre><code>/Users/waadalkatheri/Desktop/Finding-Charity-Donors-main/visuals.py:50: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown
  fig.show()
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_6aac95039b0940549e9920f6e43c56f3/018dc4163bf848690e32d20a74a891db398f6b44.png" /></p>
</div>
</div>
<div class="cell markdown">
<p>For highly-skewed feature distributions such as
<code>capital-gain</code> and <code>capital-loss</code>, it is common
practice to apply a <a
href="https://en.wikipedia.org/wiki/Data_transformation_(statistics)">logarithmic
transformation</a> on the data so that the very large and very small
values do not negatively affect the performance of a learning algorithm.
Using a logarithmic transformation significantly reduces the range of
values caused by outliers. Care must be taken when applying this
transformation however: The logarithm of 0 is undefined, so we must
translate the values by a small amount above 0 to apply the the
logarithm successfully.</p>
<p>Run the code cell below to perform a transformation on the data and
visualize the results. Again, note the range of values and how they are
distributed.</p>
</div>
<div class="cell code" data-execution_count="11">
<div class="sourceCode" id="cb17"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Log-transform the skewed features</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>skewed <span class="op">=</span> [<span class="st">&#39;capital-gain&#39;</span>, <span class="st">&#39;capital-loss&#39;</span>]</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>features_log_transformed <span class="op">=</span> pd.DataFrame(data <span class="op">=</span> X)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>features_log_transformed[skewed] <span class="op">=</span> X[skewed].<span class="bu">apply</span>(<span class="kw">lambda</span> x: np.log(x <span class="op">+</span> <span class="dv">1</span>))</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the new log distributions</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>vs.distribution(features_log_transformed, transformed <span class="op">=</span> <span class="va">True</span>)</span></code></pre></div>
<div class="output stream stderr">
<pre><code>/Users/waadalkatheri/Desktop/Finding-Charity-Donors-main/visuals.py:50: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown
  fig.show()
</code></pre>
</div>
</div>
<section id="normalizing-numerical-features" class="cell markdown">
<h3>Normalizing Numerical Features</h3>
<p>In addition to performing transformations on features that are highly
skewed, it is often good practice to perform some type of scaling on
numerical features. Applying a scaling to the data does not change the
shape of each feature's distribution (such as <code>capital-gain</code>
or <code>capital-loss</code> above); however, normalization ensures that
each feature is treated equally when applying supervised learners. Note
that once scaling is applied, observing the data in its raw form will no
longer have the same original meaning, as exampled below.</p>
<p>Run the code cell below to normalize each numerical feature. We will
use <a
href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html"><code>sklearn.preprocessing.MinMaxScaler</code></a>
for this.</p>
</section>
<div class="cell code" data-execution_count="12">
<div class="sourceCode" id="cb19"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize a scaler, then apply it to the features</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> MinMaxScaler() <span class="co"># default=(0, 1)</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>numerical <span class="op">=</span> [<span class="st">&#39;age&#39;</span>, <span class="st">&#39;education-num&#39;</span>, <span class="st">&#39;capital-gain&#39;</span>, <span class="st">&#39;capital-loss&#39;</span>, <span class="st">&#39;hours-per-week&#39;</span>]</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>features_log_minmax_transform <span class="op">=</span> pd.DataFrame(data <span class="op">=</span> features_log_transformed)</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>features_log_minmax_transform[numerical] <span class="op">=</span> scaler.fit_transform(features_log_transformed[numerical])</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Show an example of a record with scaling applied</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>display(features_log_minmax_transform.head(n <span class="op">=</span> <span class="dv">5</span>))</span></code></pre></div>
<div class="output display_data">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>workclass</th>
      <th>education_level</th>
      <th>education-num</th>
      <th>marital-status</th>
      <th>occupation</th>
      <th>relationship</th>
      <th>race</th>
      <th>sex</th>
      <th>capital-gain</th>
      <th>capital-loss</th>
      <th>hours-per-week</th>
      <th>native-country</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.301370</td>
      <td>State-gov</td>
      <td>Bachelors</td>
      <td>0.800000</td>
      <td>Never-married</td>
      <td>Adm-clerical</td>
      <td>Not-in-family</td>
      <td>White</td>
      <td>Male</td>
      <td>0.667492</td>
      <td>0.0</td>
      <td>0.397959</td>
      <td>United-States</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.452055</td>
      <td>Self-emp-not-inc</td>
      <td>Bachelors</td>
      <td>0.800000</td>
      <td>Married-civ-spouse</td>
      <td>Exec-managerial</td>
      <td>Husband</td>
      <td>White</td>
      <td>Male</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.122449</td>
      <td>United-States</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.287671</td>
      <td>Private</td>
      <td>HS-grad</td>
      <td>0.533333</td>
      <td>Divorced</td>
      <td>Handlers-cleaners</td>
      <td>Not-in-family</td>
      <td>White</td>
      <td>Male</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.397959</td>
      <td>United-States</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.493151</td>
      <td>Private</td>
      <td>11th</td>
      <td>0.400000</td>
      <td>Married-civ-spouse</td>
      <td>Handlers-cleaners</td>
      <td>Husband</td>
      <td>Black</td>
      <td>Male</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.397959</td>
      <td>United-States</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.150685</td>
      <td>Private</td>
      <td>Bachelors</td>
      <td>0.800000</td>
      <td>Married-civ-spouse</td>
      <td>Prof-specialty</td>
      <td>Wife</td>
      <td>Black</td>
      <td>Female</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.397959</td>
      <td>Cuba</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<section id="implementation-data-preprocessing" class="cell markdown">
<h3>Implementation: Data Preprocessing</h3>
<p>From the table in <strong>Exploring the Data</strong> above, we can
see there are several features for each record that are non-numeric.
Typically, learning algorithms expect input to be numeric, which
requires that non-numeric features (called <em>categorical
variables</em>) be converted. One popular way to convert categorical
variables is by using the <strong>one-hot encoding</strong> scheme.</p>
<p>Additionally, as with the non-numeric features, we need to convert
the non-numeric target label, <code>income</code> to numerical values
for the learning algorithm to work. Since there are only two possible
categories for this label ("&lt;=50K" and "&gt;50K"), we can avoid using
one-hot encoding and simply encode these two categories as 0 and 1,
respectively.</p>
</section>
<div class="cell code" data-execution_count="13">
<div class="sourceCode" id="cb20"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> encode_income(value):</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Encoding the income feature to numerical, Set records with &quot;&lt;=50K&quot; to 0 and records with &quot;&gt;50K&quot; to 1.</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="co">        value (str) either &quot;&lt;=50K&quot; or &quot;&lt;=50K&quot;</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a><span class="co">        0 if value is &quot;&lt;=50K&quot; or 1 if value is &quot;&gt;50K&quot;</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> value <span class="op">==</span> <span class="st">&quot;&lt;=50K&quot;</span>:</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="dv">0</span></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="dv">1</span></span></code></pre></div>
</div>
<div class="cell code" data-execution_count="14">
<div class="sourceCode" id="cb21"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Encode the &#39;income_raw&#39; data to numerical values</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>Y_encoded <span class="op">=</span> Y.<span class="bu">apply</span>(encode_income)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>Y_encoded.sample(<span class="dv">10</span>) <span class="co">## to ensure accurate encoding</span></span></code></pre></div>
<div class="output execute_result" data-execution_count="14">
<pre><code>3277     1
19918    0
22366    0
1418     0
1278     0
3373     1
30732    0
11647    0
7835     0
38855    0
Name: income, dtype: int64</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="15" data-scrolled="true">
<div class="sourceCode" id="cb23"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Performing One-hot encode the &#39;features_log_minmax_transform&#39; data using pandas.get_dummies()</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>X_encoded <span class="op">=</span> pd.get_dummies(features_log_minmax_transform) </span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the number of features after one-hot encoding</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>encoded <span class="op">=</span> <span class="bu">list</span>(X_encoded.columns)</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;</span><span class="sc">{}</span><span class="st"> total features after one-hot encoding.&quot;</span>.<span class="bu">format</span>(<span class="bu">len</span>(encoded)))</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Uncomment the following line to see the encoded feature names</span></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(encoded)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>103 total features after one-hot encoding.
[&#39;age&#39;, &#39;education-num&#39;, &#39;capital-gain&#39;, &#39;capital-loss&#39;, &#39;hours-per-week&#39;, &#39;workclass_ Federal-gov&#39;, &#39;workclass_ Local-gov&#39;, &#39;workclass_ Private&#39;, &#39;workclass_ Self-emp-inc&#39;, &#39;workclass_ Self-emp-not-inc&#39;, &#39;workclass_ State-gov&#39;, &#39;workclass_ Without-pay&#39;, &#39;education_level_ 10th&#39;, &#39;education_level_ 11th&#39;, &#39;education_level_ 12th&#39;, &#39;education_level_ 1st-4th&#39;, &#39;education_level_ 5th-6th&#39;, &#39;education_level_ 7th-8th&#39;, &#39;education_level_ 9th&#39;, &#39;education_level_ Assoc-acdm&#39;, &#39;education_level_ Assoc-voc&#39;, &#39;education_level_ Bachelors&#39;, &#39;education_level_ Doctorate&#39;, &#39;education_level_ HS-grad&#39;, &#39;education_level_ Masters&#39;, &#39;education_level_ Preschool&#39;, &#39;education_level_ Prof-school&#39;, &#39;education_level_ Some-college&#39;, &#39;marital-status_ Divorced&#39;, &#39;marital-status_ Married-AF-spouse&#39;, &#39;marital-status_ Married-civ-spouse&#39;, &#39;marital-status_ Married-spouse-absent&#39;, &#39;marital-status_ Never-married&#39;, &#39;marital-status_ Separated&#39;, &#39;marital-status_ Widowed&#39;, &#39;occupation_ Adm-clerical&#39;, &#39;occupation_ Armed-Forces&#39;, &#39;occupation_ Craft-repair&#39;, &#39;occupation_ Exec-managerial&#39;, &#39;occupation_ Farming-fishing&#39;, &#39;occupation_ Handlers-cleaners&#39;, &#39;occupation_ Machine-op-inspct&#39;, &#39;occupation_ Other-service&#39;, &#39;occupation_ Priv-house-serv&#39;, &#39;occupation_ Prof-specialty&#39;, &#39;occupation_ Protective-serv&#39;, &#39;occupation_ Sales&#39;, &#39;occupation_ Tech-support&#39;, &#39;occupation_ Transport-moving&#39;, &#39;relationship_ Husband&#39;, &#39;relationship_ Not-in-family&#39;, &#39;relationship_ Other-relative&#39;, &#39;relationship_ Own-child&#39;, &#39;relationship_ Unmarried&#39;, &#39;relationship_ Wife&#39;, &#39;race_ Amer-Indian-Eskimo&#39;, &#39;race_ Asian-Pac-Islander&#39;, &#39;race_ Black&#39;, &#39;race_ Other&#39;, &#39;race_ White&#39;, &#39;sex_ Female&#39;, &#39;sex_ Male&#39;, &#39;native-country_ Cambodia&#39;, &#39;native-country_ Canada&#39;, &#39;native-country_ China&#39;, &#39;native-country_ Columbia&#39;, &#39;native-country_ Cuba&#39;, &#39;native-country_ Dominican-Republic&#39;, &#39;native-country_ Ecuador&#39;, &#39;native-country_ El-Salvador&#39;, &#39;native-country_ England&#39;, &#39;native-country_ France&#39;, &#39;native-country_ Germany&#39;, &#39;native-country_ Greece&#39;, &#39;native-country_ Guatemala&#39;, &#39;native-country_ Haiti&#39;, &#39;native-country_ Holand-Netherlands&#39;, &#39;native-country_ Honduras&#39;, &#39;native-country_ Hong&#39;, &#39;native-country_ Hungary&#39;, &#39;native-country_ India&#39;, &#39;native-country_ Iran&#39;, &#39;native-country_ Ireland&#39;, &#39;native-country_ Italy&#39;, &#39;native-country_ Jamaica&#39;, &#39;native-country_ Japan&#39;, &#39;native-country_ Laos&#39;, &#39;native-country_ Mexico&#39;, &#39;native-country_ Nicaragua&#39;, &#39;native-country_ Outlying-US(Guam-USVI-etc)&#39;, &#39;native-country_ Peru&#39;, &#39;native-country_ Philippines&#39;, &#39;native-country_ Poland&#39;, &#39;native-country_ Portugal&#39;, &#39;native-country_ Puerto-Rico&#39;, &#39;native-country_ Scotland&#39;, &#39;native-country_ South&#39;, &#39;native-country_ Taiwan&#39;, &#39;native-country_ Thailand&#39;, &#39;native-country_ Trinadad&amp;Tobago&#39;, &#39;native-country_ United-States&#39;, &#39;native-country_ Vietnam&#39;, &#39;native-country_ Yugoslavia&#39;]
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="15">
<div class="sourceCode" id="cb25"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>X_encoded.sample(<span class="dv">10</span>) <span class="co">## to ensure accurate encoding let&#39;s take a look </span></span></code></pre></div>
<div class="output execute_result" data-execution_count="15">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>education-num</th>
      <th>capital-gain</th>
      <th>capital-loss</th>
      <th>hours-per-week</th>
      <th>workclass_ Federal-gov</th>
      <th>workclass_ Local-gov</th>
      <th>workclass_ Private</th>
      <th>workclass_ Self-emp-inc</th>
      <th>workclass_ Self-emp-not-inc</th>
      <th>...</th>
      <th>native-country_ Portugal</th>
      <th>native-country_ Puerto-Rico</th>
      <th>native-country_ Scotland</th>
      <th>native-country_ South</th>
      <th>native-country_ Taiwan</th>
      <th>native-country_ Thailand</th>
      <th>native-country_ Trinadad&amp;Tobago</th>
      <th>native-country_ United-States</th>
      <th>native-country_ Vietnam</th>
      <th>native-country_ Yugoslavia</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>22221</th>
      <td>0.301370</td>
      <td>0.533333</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.397959</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>...</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>12170</th>
      <td>0.205479</td>
      <td>0.533333</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.397959</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>...</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>2048</th>
      <td>0.342466</td>
      <td>0.733333</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.397959</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>...</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>17615</th>
      <td>0.054795</td>
      <td>0.333333</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.397959</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>...</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>28535</th>
      <td>0.219178</td>
      <td>0.600000</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.244898</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>...</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>837</th>
      <td>0.136986</td>
      <td>0.533333</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.295918</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>...</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>29007</th>
      <td>0.342466</td>
      <td>0.533333</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.438776</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>...</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>4612</th>
      <td>0.342466</td>
      <td>0.933333</td>
      <td>0.0</td>
      <td>0.905759</td>
      <td>0.500000</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>...</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>34694</th>
      <td>0.136986</td>
      <td>0.533333</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.397959</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>...</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>39437</th>
      <td>0.095890</td>
      <td>0.533333</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.448980</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>...</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
    </tr>
  </tbody>
</table>
<p>10 rows × 103 columns</p>
</div>
</div>
</div>
<section id="shuffle-and-split-data" class="cell markdown">
<h3>Shuffle and Split Data</h3>
<p>Now all <em>categorical variables</em> have been converted into
numerical features, and all numerical features have been normalized. As
always, we will now split the data (both features and their labels) into
training and test sets. 80% of the data will be used for training and
20% for testing.</p>
<p>Run the code cell below to perform this split.</p>
</section>
<div class="cell code" data-execution_count="16">
<div class="sourceCode" id="cb26"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the &#39;features&#39; and &#39;income&#39; data into training and testing sets</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X_encoded, Y_encoded, test_size <span class="op">=</span> <span class="fl">0.2</span>, random_state <span class="op">=</span> <span class="dv">42</span>)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="17">
<div class="sourceCode" id="cb27"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Show the results of the split</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Training set has </span><span class="sc">{}</span><span class="st"> samples.&quot;</span>.<span class="bu">format</span>(X_train.shape[<span class="dv">0</span>]))</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Testing set has </span><span class="sc">{}</span><span class="st"> samples.&quot;</span>.<span class="bu">format</span>(X_test.shape[<span class="dv">0</span>]))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Training set has 36177 samples.
Testing set has 9045 samples.
</code></pre>
</div>
</div>
<div class="cell markdown">
<hr />
<h2 id="evaluating-model-performance">Evaluating Model Performance</h2>
<p>In this section, we will investigate four different algorithms, and
determine which is best at modeling the data. Three of these algorithms
will be supervised learners:</p>
<ul>
<li><code>clf_A = KNeighborsClassifier()</code> K neighbors.</li>
<li><code>clf_B = SVC()</code> Suppportive vector</li>
<li><code>clf_C = LogisticRegression()</code> Logestic Regression</li>
</ul>
<p>and the fourth algorithm is known as a <em>naive predictor</em>.</p>
</div>
<section id="question-1---naive-predictor-performace"
class="cell markdown">
<h3>Question 1 - Naive Predictor Performace</h3>
<ul>
<li>If we chose a model that always predicted an individual made more
than $50,000, what would that model's accuracy and F-score be on this
dataset?</li>
</ul>
</section>
<div class="cell code" data-execution_count="18">
<div class="sourceCode" id="cb29"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>TP <span class="op">=</span> np.<span class="bu">sum</span>(Y_encoded) <span class="co"># Counting the ones as this is the naive case. Note that &#39;Y_encoded&#39; is the &#39;income_raw&#39; data encoded to numerical values done in the data preprocessing step.</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>FP <span class="op">=</span> Y_encoded.count() <span class="op">-</span> TP <span class="co"># Specific to the naive case</span></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>TN <span class="op">=</span> <span class="dv">0</span> <span class="co"># No predicted negatives in the naive case</span></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>FN <span class="op">=</span> <span class="dv">0</span> <span class="co"># No predicted negatives in the naive case</span></span></code></pre></div>
</div>
<div class="cell code" data-execution_count="19">
<div class="sourceCode" id="cb30"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculating accuracy, precision and recall</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> (TP <span class="op">+</span> TN) <span class="op">/</span> (TP <span class="op">+</span> TN <span class="op">+</span> FP <span class="op">+</span> FN)</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>recall <span class="op">=</span>  TP <span class="op">/</span> (TP <span class="op">+</span> FN)</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>precision <span class="op">=</span>  TP <span class="op">/</span> (TP <span class="op">+</span> FP)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="20">
<div class="sourceCode" id="cb31"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculating F-score using the formula above for beta = 0.5 and correct values for precision and recall.</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>beta <span class="op">=</span> <span class="fl">0.5</span> </span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>beta_sqr <span class="op">=</span> np.square(beta)</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>f_score <span class="op">=</span> (<span class="dv">1</span> <span class="op">+</span> beta_sqr) <span class="op">*</span> ((precision <span class="op">*</span> recall) <span class="op">/</span> ((beta_sqr <span class="op">*</span> precision) <span class="op">+</span> recall))</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="21">
<div class="sourceCode" id="cb32"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the results </span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Naive Predictor: [Accuracy score: </span><span class="sc">{:.4f}</span><span class="st">, F-score: </span><span class="sc">{:.4f}</span><span class="st">]&quot;</span>.<span class="bu">format</span>(accuracy, f_score))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Naive Predictor: [Accuracy score: 0.2478, F-score: 0.2917]
</code></pre>
</div>
</div>
<div class="cell markdown">
<blockquote>
<p><strong>Answer:</strong></p>
</blockquote>
<p>The naive predictor is a simple baseline model that always predicts a
single outcome without considering the data. In this case, the naive
predictor assumes that every individual earns more than $50K. Based on
the calculations, the accuracy of this model is 0.2478, meaning it is
correct only 24.78% of the time. The F-score, which balances precision
and recall with a beta of 0.5, is calculated as 0.2917.</p>
<ul>
<li>When the model always predicts "1" (i.e., the individual earns more
than $50K), there are no True Negatives (TN) or False Negatives (FN)
because the model does not make any negative predictions (0 values). As
a result, the accuracy in this scenario is equivalent to the precision,
calculated as True Positives divided by the total number of predictions
(True Positives + False Positives). Every incorrect prediction of "1"
where the true label is "0" becomes a False Positive, making the
denominator equal to the total number of samples. Recall, on the other
hand, is 1.0 because there are no False Negatives, meaning all positive
cases in the data are captured by the predictions.</li>
</ul>
</div>
<section id="question-2---model-application" class="cell markdown">
<h3>Question 2 - Model Application</h3>
<p>List three of the supervised learning models above that are
appropriate for this problem that you will test on the census data. For
each model chosen</p>
<ul>
<li>Describe one real-world application in industry where the model can
be applied.</li>
<li>What are the strengths of the model; when does it perform well?</li>
<li>What are the weaknesses of the model; when does it perform
poorly?</li>
<li>What makes this model a good candidate for the problem, given what
you know about the data?</li>
</ul>
</section>
<div class="cell markdown">
<blockquote>
<p><strong>Answer:</strong></p>
</blockquote>
<p>The three supervised learning models used are:</p>
<ul>
<li><p><strong>K-Nearest Neighbors</strong></p></li>
<li><p>Real-World Applications:</p></li>
<li><p>Enhances investment decisions in finance.</p></li>
<li><p>Detects diseases early in healthcare.</p></li>
<li><p>Improves sales and user satisfaction in e-commerce.</p></li>
<li><p>Strengthens security and automation in computer vision.</p></li>
<li><p>Strengths:</p></li>
<li><p>Simple to implement and computationally fast.</p></li>
<li><p>Adapts easily to new data and generalizes well.</p></li>
<li><p>Requires fewer hyperparameters compared to other
algorithms.</p></li>
<li><p>Weaknesses:</p></li>
<li><p>Struggles with high-dimensional data due to the curse of
dimensionality.</p></li>
<li><p>Prone to overfitting if the value of k is not chosen
carefully.</p></li>
<li><p>Reason for Choosing:</p></li>
<li><p>KNN is effective for classification tasks involving
similarity-based decisions. Since this problem involves predicting
potential donors, KNN can leverage the similarities in data to make
accurate predictions. Its adaptability to new data makes it suitable for
this task.</p></li>
<li><p><strong>Support vector Machine</strong></p></li>
<li><p>Real-World Applications:</p></li>
<li><p>Face detection by classifying face vs. non-face regions.</p></li>
<li><p>Document categorization in text and hypertext.</p></li>
<li><p>Gene classification and cancer detection in
bioinformatics.</p></li>
<li><p>Widely used for handwriting recognition.</p></li>
<li><p>Strengths:</p></li>
<li><p>Works well when there is a clear margin of separation between
classes.</p></li>
<li><p>Effective in high-dimensional spaces. -Performs well when the
number of features exceeds the number of samples.</p></li>
<li><p>Memory-efficient.</p></li>
<li><p>Weaknesses:</p></li>
<li><p>Not suitable for very large datasets.</p></li>
<li><p>Performs poorly with noisy data or overlapping classes.</p></li>
<li><p>May underfit when the number of features far exceeds the training
samples.</p></li>
<li><p>Reason for Choosing:</p></li>
<li><p>The dataset, after preprocessing and encoding, is
high-dimensional. SVM is ideal for binary classification tasks in such
scenarios as it constructs decision boundaries to separate potential
donors effectively.</p></li>
<li><p><strong>Logistic Regression</strong></p></li>
<li><p>Real-World Applications:</p></li>
<li><p>Used in credit scoring with remarkable results.</p></li>
<li><p>Identifies relationships between micro-RNA and genes in
medicine.</p></li>
<li><p>Processes and formats text data in text editors.</p></li>
<li><p>Predicts user behavior, such as changes in journey dates, in the
travel industry.</p></li>
<li><p>Strengths:</p></li>
<li><p>Provides outputs with probabilistic interpretation.</p></li>
<li><p>Regularization techniques prevent overfitting.</p></li>
<li><p>Easily updated with new data using stochastic gradient
descent.</p></li>
<li><p>Weaknesses:</p></li>
<li><p>Underperforms with multiple or non-linear decision
boundaries.</p></li>
<li><p>Not flexible enough to naturally capture complex
relationships.</p></li>
<li><p>Reason for Choosing: Logistic Regression works well for binary
classification problems. It is simple, interpretable, and less prone to
overfitting, making it an efficient choice for separating the data and
predicting donor behavior.</p></li>
</ul>
</div>
<section
id="implementation---creating-a-training-and-predicting-pipeline"
class="cell markdown">
<h3>Implementation - Creating a Training and Predicting Pipeline</h3>
<p>To properly evaluate the performance of each model I've chosen, it's
important that we create a training and predicting pipeline that allows
you to quickly and effectively train models using various sizes of
training data and perform predictions on the testing data.</p>
</section>
<div class="cell code" data-execution_count="22">
<div class="sourceCode" id="cb34"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_predict(learner, sample_size, X_train, y_train, X_test, y_test): </span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>   <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a><span class="co">   Trains a model and predicts to return the results of performance metrics.</span></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a><span class="co">   Args:</span></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a><span class="co">      - learner: the learning algorithm to be trained and predicted on</span></span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a><span class="co">      - sample_size: the size of samples (number) to be drawn from training set</span></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a><span class="co">      - X_train: features training set</span></span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a><span class="co">      - y_train: income training set</span></span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a><span class="co">      - X_test: features testing set</span></span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a><span class="co">      - y_test: income testing set</span></span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a><span class="co">   Returns:</span></span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a><span class="co">      results (set)</span></span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a><span class="co">   &quot;&quot;&quot;</span></span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a>   results <span class="op">=</span> {}</span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a>   <span class="co"># Fit the learner to the training data using slicing with &#39;sample_size&#39; using .fit(training_features[:], training_labels[:])</span></span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a>   train_start <span class="op">=</span> time() <span class="co"># Get start time</span></span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a>   learner <span class="op">=</span> learner.fit(X_train[:sample_size], y_train[:sample_size])</span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a>   train_end <span class="op">=</span> time() <span class="co"># Get end time</span></span>
<span id="cb34-21"><a href="#cb34-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-22"><a href="#cb34-22" aria-hidden="true" tabindex="-1"></a>   <span class="co"># Get the predictions on the test set(X_test), then get predictions on the first 300 training samples(X_train) using .predict()</span></span>
<span id="cb34-23"><a href="#cb34-23" aria-hidden="true" tabindex="-1"></a>   pred_start <span class="op">=</span> time() <span class="co"># Get start time</span></span>
<span id="cb34-24"><a href="#cb34-24" aria-hidden="true" tabindex="-1"></a>   predictions_test <span class="op">=</span> learner.predict(X_test)</span>
<span id="cb34-25"><a href="#cb34-25" aria-hidden="true" tabindex="-1"></a>   predictions_train <span class="op">=</span> learner.predict(X_train[:<span class="dv">300</span>])</span>
<span id="cb34-26"><a href="#cb34-26" aria-hidden="true" tabindex="-1"></a>   pred_end <span class="op">=</span> time() <span class="co"># Get end time</span></span>
<span id="cb34-27"><a href="#cb34-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-28"><a href="#cb34-28" aria-hidden="true" tabindex="-1"></a>   results[<span class="st">&#39;train_time&#39;</span>] <span class="op">=</span> train_end <span class="op">-</span>  train_start <span class="co"># Calculate the training time</span></span>
<span id="cb34-29"><a href="#cb34-29" aria-hidden="true" tabindex="-1"></a>   results[<span class="st">&#39;pred_time&#39;</span>] <span class="op">=</span> pred_end <span class="op">-</span> pred_start   <span class="co"># Calculate the total prediction time</span></span>
<span id="cb34-30"><a href="#cb34-30" aria-hidden="true" tabindex="-1"></a>   results[<span class="st">&#39;acc_train&#39;</span>] <span class="op">=</span> accuracy_score(y_train[:<span class="dv">300</span>], predictions_train) <span class="co"># Compute accuracy on the first 300 training samples which is y_train[:300]</span></span>
<span id="cb34-31"><a href="#cb34-31" aria-hidden="true" tabindex="-1"></a>   results[<span class="st">&#39;acc_test&#39;</span>] <span class="op">=</span> accuracy_score(y_test, predictions_test) <span class="co"># Compute accuracy on test set using accuracy_score()</span></span>
<span id="cb34-32"><a href="#cb34-32" aria-hidden="true" tabindex="-1"></a>   results[<span class="st">&#39;f_train&#39;</span>] <span class="op">=</span> fbeta_score(y_train[:<span class="dv">300</span>], predictions_train,  beta <span class="op">=</span> <span class="fl">0.5</span>)    <span class="co"># Compute F-score on the the first 300 training samples using fbeta_score()</span></span>
<span id="cb34-33"><a href="#cb34-33" aria-hidden="true" tabindex="-1"></a>   results[<span class="st">&#39;f_test&#39;</span>] <span class="op">=</span> fbeta_score(y_test, predictions_test, beta <span class="op">=</span> <span class="fl">0.5</span>) <span class="co"># Compute F-score on the test set which is y_test</span></span>
<span id="cb34-34"><a href="#cb34-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-35"><a href="#cb34-35" aria-hidden="true" tabindex="-1"></a>   <span class="co"># Success</span></span>
<span id="cb34-36"><a href="#cb34-36" aria-hidden="true" tabindex="-1"></a>   <span class="bu">print</span>(<span class="st">&quot;</span><span class="sc">{}</span><span class="st"> trained on </span><span class="sc">{}</span><span class="st"> samples.&quot;</span>.<span class="bu">format</span>(learner.__class__.<span class="va">__name__</span>, sample_size))</span>
<span id="cb34-37"><a href="#cb34-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-38"><a href="#cb34-38" aria-hidden="true" tabindex="-1"></a>   <span class="cf">return</span> results  <span class="co"># Return the results</span></span></code></pre></div>
</div>
<section id="implementation-initial-model-evaluation"
class="cell markdown">
<h3>Implementation: Initial Model Evaluation</h3>
<p>In this section, the default settings for each model are used, then
one specific model is tuned in a later section.</p>
</section>
<div class="cell code" data-execution_count="23" data-collapsed="true"
data-jupyter="{&quot;outputs_hidden&quot;:true}">
<div class="sourceCode" id="cb35"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the three models</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>clf_A <span class="op">=</span> KNeighborsClassifier()</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>clf_B <span class="op">=</span> SVC(random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>clf_C <span class="op">=</span> LogisticRegression(random_state<span class="op">=</span><span class="dv">42</span>)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="24">
<div class="sourceCode" id="cb36"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculating the number of samples for 1%, 10%, and 100% of the training data, samples_1 is 1% of samples_100 ( the count of the values is set to be `int` and not `float`)</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>samples_100 <span class="op">=</span> <span class="bu">len</span>(y_train)</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>samples_10 <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.1</span><span class="op">*</span>samples_100)</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>samples_1 <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.01</span><span class="op">*</span>samples_100) </span></code></pre></div>
</div>
<div class="cell code" data-execution_count="25">
<div class="sourceCode" id="cb37"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Collect results on the learners</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> {}</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> clf <span class="kw">in</span> [clf_A, clf_B, clf_C]:</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>    clf_name <span class="op">=</span> clf.__class__.<span class="va">__name__</span></span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>    results[clf_name] <span class="op">=</span> {}</span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, samples <span class="kw">in</span> <span class="bu">enumerate</span>([samples_1, samples_10, samples_100]):</span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>        results[clf_name][i] <span class="op">=</span> <span class="op">\</span></span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a>        train_predict(clf, samples, X_train, y_train, X_test, y_test)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>KNeighborsClassifier trained on 361 samples.
KNeighborsClassifier trained on 3617 samples.
KNeighborsClassifier trained on 36177 samples.
SVC trained on 361 samples.
SVC trained on 3617 samples.
SVC trained on 36177 samples.
LogisticRegression trained on 361 samples.
LogisticRegression trained on 3617 samples.
LogisticRegression trained on 36177 samples.
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="26">
<div class="sourceCode" id="cb39"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Run metrics visualization for the three supervised learning models chosen</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>vs.evaluate(results, accuracy, f_score)</span></code></pre></div>
<div class="output stream stderr">
<pre><code>/Users/aqlamaai/Documents/Finding-Charity-Donors/visuals.py:122: UserWarning: Tight layout not applied. tight_layout cannot make Axes width small enough to accommodate all Axes decorations
  pl.tight_layout()
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_6aac95039b0940549e9920f6e43c56f3/d1ff32aeb6b88c8a68369b3d311e1ae7eda2a392.png" /></p>
</div>
</div>
<div class="cell markdown">
<p><strong>interpretations on performance:</strong></p>
<blockquote>
<p>Model training time:</p>
</blockquote>
<ul>
<li>SVC takes the longer time to train, specialy when the training set
is all used 100%. that seems to be computationally expensive.</li>
<li>The KNN and the LR take much less time in training overall samples
(1%, 10%, 100%)</li>
</ul>
<blockquote>
<p>Accuracy Score on training:</p>
</blockquote>
<ul>
<li>All the algorthims acheive a high training accuracy on all subsets.
LR seems to have almost the same accuracies on all subestes. The highest
accuracy scored when the KNN model is trained on 100% of the training
set.</li>
<li>SVC performs the same on all training subsets.</li>
</ul>
<blockquote>
<p>F-score on training:</p>
</blockquote>
<ul>
<li>they seem to score similarly high across all training subsets. That
means there is a good balance between percision and recall
training.</li>
</ul>
<blockquote>
<p>Model Prediction Times:</p>
</blockquote>
<ul>
<li>SVM is the sloweset in making predictions, with the it's increasing
hugely when the whole training set is used as it reaches almost 14
seconds.</li>
<li>KNN time to predict also slightly increases when the training subset
increases.</li>
<li>As for the logestic regresion, is is the fastest in prediction time,
alsmost no time in comparisio to the other two algorthims.</li>
</ul>
<blockquote>
<p>Accuracy score on testing:</p>
</blockquote>
<ul>
<li>Models have almost the same performance, within each subset of the
training samples. and within the same range of training.</li>
<li>That is the models are genralizing well on the unseen data, non of
the algorthims have the overfitting issue.</li>
<li>The highest score on testing accuracy is the SVC model.</li>
</ul>
<blockquote>
<p>F-score on testing:</p>
</blockquote>
<ul>
<li>This scores drops from 0.8 to 0.6 for all models.</li>
</ul>
</div>
<div class="cell markdown">
<hr />
<h2 id="improving-results">Improving Results</h2>
<p>In this final section, I will choose from the three supervised
learning models the <em>best</em> model to use on the data. then I
perform a grid search optimization for the model over the entire
training set (<code>X_train</code> and <code>y_train</code>) by tuning
at least one parameter to improve upon the untuned model's F-score.</p>
</div>
<section id="question-3---choosing-the-best-model"
class="cell markdown">
<h3>Question 3 - Choosing the Best Model</h3>
<ul>
<li>Based on the evaluation performed earlier,here is an explaination to
<em>CharityML</em> which of the three models I believe to be most
appropriate for the task of identifying individuals that make more than
$50,000.</li>
</ul>
</section>
<div class="cell markdown">
<blockquote>
<p><strong>Answer:</strong></p>
</blockquote>
<p>Based on the graph above, the model with the highest F-score on the
testing set, when using 100% of the training data, is the SVC model.</p>
<ul>
<li>The SVC model demonstrates the best balance between precision and
recall, achieving the highest F-score on the testing set.</li>
<li>However, while the SVC model excels in F-score and accuracy, it also
has the highest prediction and training times, making it computationally
expensive.</li>
<li>Considering the algorithm's suitability for the data, the high
computational cost makes SVC less practical for this task.</li>
<li>This scenario presents a trade-off between optimal performance and
computational efficiency. Given this trade-off, the Logistic Regression
(LR) model is a more suitable choice. While its F-score and accuracy are
slightly lower than the SVC, it is computationally efficient and better
aligned with the practical needs of CharityML.</li>
</ul>
</div>
<section id="question-4---describing-the-model-in-laymans-terms"
class="cell markdown">
<h3>Question 4 - Describing the Model in Layman's Terms</h3>
<ul>
<li>Here is a paragraph to explain to <em>CharityML</em>, in layman's
terms, how the final model chosen is supposed to work. describing the
major qualities of the model, such as how the model is trained and how
the model makes a prediction.</li>
</ul>
</section>
<div class="cell markdown">
<p><strong>Answer:</strong></p>
<p>The Logistic Regression (LR) model is chosen for this task because it
is straightforward and effective in predicting the outcome. The model
learns to identify patterns in the data by analyzing the various
features and their relationship to the target variable. Based on this,
it can evaluate new data and predict whether a person is likely to
donate.</p>
<p>The model works by assigning weights to each feature and combining
them into a single value. If this value exceeds a certain threshold, the
prediction will be that the person earns more than $50K; otherwise, the
prediction will be that their income is less than $50K. Its simplicity
and speed make it a practical and reliable solution for this
problem.</p>
</div>
<section id="implementation-model-tuning" class="cell markdown">
<h3>Implementation: Model Tuning</h3>
<p>In this section we Fine tune the LR model. Using grid search
(<code>GridSearchCV</code>) with at least one important parameter tuned
with at least 3 different values. Using the entire training set for
this.</p>
</section>
<div class="cell code" data-execution_count="24">
<div class="sourceCode" id="cb41"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the classifier</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> LogisticRegression()</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="25">
<div class="sourceCode" id="cb42"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Creating the parameters list to tune, using a dictionary </span></span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a><span class="co">## C is the regularization strength</span></span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a><span class="co">## penalty is the regularization type</span></span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a><span class="co">## solver has to match the penalty</span></span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a><span class="co">## max_iter is set because I got a  ConvergenceWarning: The max_iter was reached which means the coef_ did not converge</span></span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a>parameters <span class="op">=</span> {</span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;C&#39;</span>: [<span class="fl">0.1</span>, <span class="dv">1</span>, <span class="dv">10</span>],</span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;penalty&#39;</span>: [<span class="st">&#39;l1&#39;</span>, <span class="st">&#39;l2&#39;</span>],</span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;solver&#39;</span>: [<span class="st">&#39;liblinear&#39;</span>, <span class="st">&#39;saga&#39;</span>],</span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;max_iter&#39;</span>: [<span class="dv">1000</span>]</span>
<span id="cb42-12"><a href="#cb42-12" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="26">
<div class="sourceCode" id="cb43"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Make an fbeta_score scoring object using make_scorer()</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>scorer <span class="op">=</span> make_scorer(fbeta_score, beta <span class="op">=</span> <span class="fl">0.5</span>)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="27">
<div class="sourceCode" id="cb44"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="co">## Performing grid search on the classifier using &#39;scorer&#39; as the scoring method using GridSearchCV()</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>grid_obj <span class="op">=</span> GridSearchCV(estimator<span class="op">=</span> clf, param_grid<span class="op">=</span> parameters, scoring<span class="op">=</span> scorer, cv <span class="op">=</span> <span class="dv">5</span>)</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a><span class="co">## Fiting the grid search object to the training data to find the optimal parameters using fit()</span></span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a>grid_fit <span class="op">=</span> grid_obj.fit(X_train, y_train)</span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a><span class="co">## Geting the estimator</span></span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a>best_clf <span class="op">=</span> grid_fit.best_estimator_</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="28">
<div class="sourceCode" id="cb45"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Make predictions using the unoptimized and model</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> (clf.fit(X_train, y_train)).predict(X_test)</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>best_predictions <span class="op">=</span> best_clf.predict(X_test)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="30">
<div class="sourceCode" id="cb46"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Report the before-and-afterscores</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Unoptimized model</span><span class="ch">\n</span><span class="st">------&quot;</span>)</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Accuracy score on testing data: </span><span class="sc">{:.4f}</span><span class="st">&quot;</span>.<span class="bu">format</span>(accuracy_score(y_test, predictions)))</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;F-score on testing data: </span><span class="sc">{:.4f}</span><span class="st">&quot;</span>.<span class="bu">format</span>(fbeta_score(y_test, predictions, beta <span class="op">=</span> <span class="fl">0.5</span>)))</span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Optimized Model</span><span class="ch">\n</span><span class="st">------&quot;</span>)</span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Final accuracy score on the testing data: </span><span class="sc">{:.4f}</span><span class="st">&quot;</span>.<span class="bu">format</span>(accuracy_score(y_test, best_predictions)))</span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Final F-score on the testing data: </span><span class="sc">{:.4f}</span><span class="st">&quot;</span>.<span class="bu">format</span>(fbeta_score(y_test, best_predictions, beta <span class="op">=</span> <span class="fl">0.5</span>)))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Unoptimized model
------
Accuracy score on testing data: 0.8456
F-score on testing data: 0.7103

Optimized Model
------
Final accuracy score on the testing data: 0.8449
Final F-score on the testing data: 0.7092
</code></pre>
</div>
</div>
<section id="question-5---final-model-evaluation" class="cell markdown">
<h3>Question 5 - Final Model Evaluation</h3>
<p>The table below shows the results of the optimzation done on the LR
model.</p>
</section>
<section id="results" class="cell markdown">
<h4>Results:</h4>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Metric</th>
<th style="text-align: center;">Unoptimized Model</th>
<th style="text-align: center;">Optimized Model</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Accuracy Score</td>
<td style="text-align: center;">0.8456</td>
<td style="text-align: center;">0.8449</td>
</tr>
<tr class="even">
<td style="text-align: center;">F-score</td>
<td style="text-align: center;">0.7103</td>
<td style="text-align: center;">0.7092</td>
</tr>
</tbody>
</table>
</section>
<div class="cell markdown">
<p><strong>Answer:</strong></p>
<p>When comparing the optimized model to the unoptimized one, there are
only minor differences in performance. The optimized model shows a
slight improvement in both accuracy and F-score, though the gains are
minimal.</p>
<p>However, when comparing the optimized model to the naive predictor,
the difference is significant. The naive predictor achieves an accuracy
of only 0.2478, whereas the optimized model reaches an accuracy of
0.8449. Similarly, the F-score of the optimized model is considerably
higher, demonstrating its superior performance over the naive
baseline.</p>
</div>
<div class="cell markdown">
<hr />
<h2 id="feature-importance">Feature Importance</h2>
<p>An important task when performing supervised learning on a dataset
like the census data we study here is determining which features provide
the most predictive power. By focusing on the relationship between only
a few crucial features and the target label we simplify our
understanding of the phenomenon, which is most always a useful thing to
do.</p>
<p>In the case of this project, that means we wish to identify a small
number of features that most strongly predict whether an individual
makes at most or more than $50,000.</p>
<p>A scikit-learn classifier that is random forests has a
<code>feature_importance_</code> attribute, which is a function that
ranks the importance of features according to the chosen classifier.</p>
<p>I fit this classifier to training set and use this attribute to
determine the top 5 most important features for the census dataset.</p>
</div>
<section id="question-6---feature-relevance-observation"
class="cell markdown">
<h3>Question 6 - Feature Relevance Observation</h3>
<p>When <strong>Exploring the Data</strong>, it was shown there are
thirteen available features for each individual on record in the census
data.</p>
<p>Of these thirteen records, let's determine which five features do you
believe to be most important for prediction, and in what order would I
rank them and why?</p>
</section>
<div class="cell code" data-execution_count="33">
<div class="sourceCode" id="cb48"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>df.columns</span></code></pre></div>
<div class="output execute_result" data-execution_count="33">
<pre><code>Index([&#39;age&#39;, &#39;workclass&#39;, &#39;education_level&#39;, &#39;education-num&#39;,
       &#39;marital-status&#39;, &#39;occupation&#39;, &#39;relationship&#39;, &#39;race&#39;, &#39;sex&#39;,
       &#39;capital-gain&#39;, &#39;capital-loss&#39;, &#39;hours-per-week&#39;, &#39;native-country&#39;,
       &#39;income&#39;],
      dtype=&#39;object&#39;)</code></pre>
</div>
</div>
<div class="cell markdown">
<p><strong>Answer:</strong></p>
<p>Based on the important features, I would select the following five as
the most significant for prediction, listed in order of relevance, along
with their reasoning:</p>
<ul>
<li><code>occupation</code>: Individuals in higher-level or specialized
roles within a company typically earn higher incomes.</li>
<li><code>workclass</code>: The type of employment or sector often
influences earning potential significantly.</li>
<li><code>marital-status</code>: Married individuals may have different
spending patterns, potentially impacting disposable income levels.</li>
<li><code>hours-per-week</code>: The number of hours worked directly
contributes to the total annual income, with more hours generally
leading to higher earnings.</li>
<li><code>education_level</code>: Higher levels of education are often
associated with better-paying jobs and increased earning capacity.</li>
</ul>
</div>
<section id="implementation---extracting-feature-importance"
class="cell markdown">
<h3>Implementation - Extracting Feature Importance</h3>
<p>A <code>scikit-learn</code> supervised learning algorithm that has a
<code>feature_importance_</code> attribute availble for it would be the
RandomForest classifier. This attribute is a function that ranks the
importance of each feature when making predictions based on the chosen
algorithm.</p>
</section>
<div class="cell code" data-execution_count="31">
<div class="sourceCode" id="cb50"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="co">## Training the supervised model on the training set using .fit(X_train, y_train)</span></span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> RandomForestClassifier()</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> model.fit(X_train, y_train)</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a><span class="co">## Extract the feature importances using .feature_importances_ </span></span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a>importances <span class="op">=</span> model.feature_importances_</span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a>vs.feature_plot(importances, X_train, y_train)</span></code></pre></div>
<div class="output display_data">
<p><img
src="vertopal_6aac95039b0940549e9920f6e43c56f3/f89a48aa4dcf7414087652b4c4a723622ffb0100.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_6aac95039b0940549e9920f6e43c56f3/ff233a9af3e51df03feefb994c16a7f4a11487dd.png" /></p>
</div>
</div>
<div class="cell markdown">
<blockquote>
<p>Interprestions:</p>
</blockquote>
<ul>
<li>Education_num feature: has the highest weight overall features. with
0.6 contribution.</li>
<li>Martial- status: comes a secnd weight with 0.6 weight.</li>
<li>Capital-gain: between 0.5 and 0.4.</li>
<li>Hours-per week: between 0.3 and 0.4</li>
<li>then finally comes the age where it has a contribution of wight
between 0.2 and 0.3/</li>
</ul>
</div>
<section id="question-7---extracting-feature-importance"
class="cell markdown">
<h3>Question 7 - Extracting Feature Importance</h3>
<p>Observing the visualization created above which displays the five
most relevant features for predicting if an individual makes at most or
above $50,000. Let's discuss how my answer compares to the actual
important features.</p>
</section>
<div class="cell markdown">
<blockquote>
<p><strong>Answer:</strong></p>
</blockquote>
<ul>
<li><p>My selection closely matched the actual feature importance, with
a few differences. Specifically, occupation and workclass did not carry
as much weight as I initially anticipated.</p></li>
<li><p>The predictions for marital status and hours-per-week were
accurate, as their significance was confirmed by the visualization.
Marital status has a weight of 0.5 or higher, and hours-per-week
contributes significantly with weights of 0.3 or more.</p></li>
<li><p>Interestingly, education level did not appear as an important
feature in the visualization. Instead, education num emerged as the most
influential feature overall, reinforcing the idea that education plays a
critical role in determining income.</p></li>
</ul>
</div>
<section id="feature-selection" class="cell markdown">
<h3>Feature Selection</h3>
<p>How does a model perform if we only use a subset of all the available
features in the data? With less features required to train, the
expectation is that training and prediction time is much lower — at the
cost of performance metrics.</p>
<p>From the visualization above, we see that the top five most important
features contribute more than half of the importance of
<strong>all</strong> features present in the data. This hints that we
can attempt to <em>reduce the feature space</em> and simplify the
information required for the model to learn.</p>
<p>Let's use the same optimized model found earlier, and train it on the
same training set <em>with only the top five important
features</em>.</p>
</section>
<div class="cell code" data-execution_count="32">
<div class="sourceCode" id="cb51"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Reduce the feature space</span></span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>X_train_reduced <span class="op">=</span> X_train[X_train.columns.values[(np.argsort(importances)[::<span class="op">-</span><span class="dv">1</span>])[:<span class="dv">5</span>]]]</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>X_test_reduced <span class="op">=</span> X_test[X_test.columns.values[(np.argsort(importances)[::<span class="op">-</span><span class="dv">1</span>])[:<span class="dv">5</span>]]]</span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Train on the &quot;best&quot; model found from grid search earlier</span></span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> (clone(best_clf)).fit(X_train_reduced, y_train)</span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Make new predictions</span></span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a>reduced_predictions <span class="op">=</span> clf.predict(X_test_reduced)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="33">
<div class="sourceCode" id="cb52"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Report scores from the final model using both versions of data</span></span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Final Model trained on full data</span><span class="ch">\n</span><span class="st">------&quot;</span>)</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Accuracy on testing data: </span><span class="sc">{:.4f}</span><span class="st">&quot;</span>.<span class="bu">format</span>(accuracy_score(y_test, best_predictions)))</span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;F-score on testing data: </span><span class="sc">{:.4f}</span><span class="st">&quot;</span>.<span class="bu">format</span>(fbeta_score(y_test, best_predictions, beta <span class="op">=</span> <span class="fl">0.5</span>)))</span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Final Model trained on reduced data</span><span class="ch">\n</span><span class="st">------&quot;</span>)</span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Accuracy on testing data: </span><span class="sc">{:.4f}</span><span class="st">&quot;</span>.<span class="bu">format</span>(accuracy_score(y_test, reduced_predictions)))</span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;F-score on testing data: </span><span class="sc">{:.4f}</span><span class="st">&quot;</span>.<span class="bu">format</span>(fbeta_score(y_test, reduced_predictions, beta <span class="op">=</span> <span class="fl">0.5</span>)))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Final Model trained on full data
------
Accuracy on testing data: 0.8449
F-score on testing data: 0.7092

Final Model trained on reduced data
------
Accuracy on testing data: 0.8284
F-score on testing data: 0.6731
</code></pre>
</div>
</div>
<div class="cell markdown">
<blockquote>
<p>Results:</p>
</blockquote>
<table>
<thead>
<tr class="header">
<th>Metric</th>
<th>Final Model (Full Data)</th>
<th>Final Model (Reduced Data)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Accuracy</strong> =</td>
<td>0.8449</td>
<td>0.8284</td>
</tr>
<tr class="even">
<td><strong>F-score</strong></td>
<td>0.7092</td>
<td>0.6731</td>
</tr>
</tbody>
</table>
</div>
<section id="question-8---effects-of-feature-selection"
class="cell markdown">
<h3>Question 8 - Effects of Feature Selection</h3>
<ul>
<li>How does the final model's F-score and accuracy score on the reduced
data using only five features compare to those same scores when all
features are used?</li>
<li>If training time was a factor, would you consider using the reduced
data as your training set?</li>
</ul>
</section>
<div class="cell markdown">
<blockquote>
<p><strong>Answer:</strong></p>
</blockquote>
<ul>
<li><p>The final model trained on the complete dataset (using all
features) achieves slightly better performance, with an accuracy of
0.8449 compared to 0.8284 and an F-score of 0.7092 compared to 0.6731,
when compared to the model trained on the reduced dataset with only five
features.</p></li>
<li><p>If training time were a significant consideration, I would opt
for the reduced dataset. While it results in a small decrease in
accuracy and F-score, using fewer features would require less
computational power and resources, making it a more efficient choice in
resource-constrained scenarios.</p></li>
</ul>
</div>
</body>
</html>
